---
title: "EJA population genomics"
author: "Arthur Oul√®s"
date: "2/1/2023"
output: html_document
---

# Load packages

```{r include=FALSE}
#Mandatory
library(here)

#Data analysis
library(pcadapt)
library(vcfR)
library(adegenet)
library(ape)

#External data acquisition for maps
library(ncdf4)
library(rnaturalearth)
library(rnaturalearthdata)
library(rnaturalearthhires)
library(marmap)

#External data acquisition for outliers BLAST
library(rentrez) #Allows to navigate NCBI database
library(xml2) #Allows xml file manipulation

#Making graph
library(ggplot2)
library(sf)
library(ggrepel)
library(ggpubr)
library(ggspatial)
library(RColorBrewer)
library(viridis)
library(scales)
library(ggnewscale)
```

# pcadapt analysis

```{r pcadapt pre-analysis to chose K value for outliers selection}
EJA_all_loci <- read.pcadapt(here("ustacks_populations_output", "all_loci", "populations.snps.vcf"), type = "vcf") #Read vcf file for every loci

pcadapt_all_loci_raw <- pcadapt(input = EJA_all_loci, K = 20) #pcadapt analysis with high value of K
plot(pcadapt_all_loci_raw, option = "screeplot") #screeplot to visually chose pca component to keep

##K = 3 seems to be the most optimal for this data set
```

```{r Analysis with optimal value of K}
pcadapt_all_loci <- pcadapt(input = EJA_all_loci, K = 3)

poplist.names <- c(
  rep("carmel", 6),
  rep("monterey", 18),
  rep("palos_verdes", 9),
  rep("laguna", 9),
  rep("san_diego", 22),
  rep("catalina", 8),
  rep("punta_banda", 20),
  rep("cardon", 3)
) #map individuals to populations

plot(pcadapt_all_loci, option = "scores", pop = poplist.names, i = 1, j = 2)
plot(pcadapt_all_loci, option = "scores", pop = poplist.names, i = 2, j = 3)
plot(pcadapt_all_loci, option = "scores", pop = poplist.names, i = 1, j = 3)

plot(pcadapt_all_loci, option = "qqplot")
hist(pcadapt_all_loci$pvalues, xlab = "p-values", main = NULL, breaks = 50, col = "orange")
```

```{r Getting outliers}
#Getting catalog rank to vcf correspondence
EJA_all_loci_vcf <- read.vcfR(here("ustacks_populations_output", "all_loci", "populations.snps.vcf"))
vcf_to_catalog_ranks.raw <- getID(EJA_all_loci_vcf) #get vcf catalog ranks
vcf_to_catalog_ranks.raw <- strsplit(x = vcf_to_catalog_ranks.raw, split = ':') #extract catalog ranks from other info

vcf_to_catalog_ranks <- rep(NA, length(vcf_to_catalog_ranks.raw))
for (i in 1:length(vcf_to_catalog_ranks)) vcf_to_catalog_ranks[i] <- vcf_to_catalog_ranks.raw[[i]][1]
vcf_to_catalog_ranks <- as.numeric(vcf_to_catalog_ranks)


#Outliers selection by pcadapt pvalues
EJA_all_loci_padj <- p.adjust(pcadapt_all_loci$pvalues, method = "bonferroni")
alpha <- 0.1 #alpha chosen following https://bcm-uga.github.io/pcadapt/articles/pcadapt.html recommendations
EJA_outliers_vcf_ranks <- which(EJA_all_loci_padj < alpha)

#Getting phi_st from population analysis
phi_st <- read.table(
  file             = here('ustacks_populations_output', 'all_loci', 'populations.phistats.tsv'),
  header           = FALSE,
  sep              = '\t',
  skip             = 9,
  stringsAsFactors = FALSE
)[,c(1,4)]
colnames(phi_st) <- c('catalog_rank', 'phi_st')

#Getting outliers catalog ranks
EJA_outliers_catalog_ranks <- vcf_to_catalog_ranks[EJA_outliers_vcf_ranks]

#Separating balancing selection from local adaptation (sorting outliers by phi_st)
EJA_outliers_phi_st <- phi_st[phi_st$catalog_rank %in% EJA_outliers_catalog_ranks, ]
dim(EJA_outliers_phi_st)[1]
EJA_outliers_phi_st_filtered <- EJA_outliers_phi_st[EJA_outliers_phi_st$phi_st > .1, ]
dim(EJA_outliers_phi_st_filtered)[1]


#Saving outliers list as txt file for stacks2 whitelist analysis
sink(here('output', 'EJA_outliers_catalog_ranks.txt'))
for (outlier in EJA_outliers_catalog_ranks){
  cat(paste0(outlier, '\n'), append = TRUE)
}
sink()
```

```{r pcadapt analysis with blacklist (neutral loci)}
EJA_blacklist <- read.pcadapt(
  here("ustacks_populations_output", "EJA_blacklist", "populations.snps.vcf"),
  type = "vcf"
)
pcadapt_blacklist_raw <- pcadapt(input = EJA_blacklist, K = 20)

plot(pcadapt_blacklist_raw, option = "screeplot")
plot(pcadapt_blacklist_raw, option = "scores", pop = poplist.names, i = 1, j = 2)
```

```{r pcadapt analysis with whitelist (outliers loci)}
EJA_whitelist <- read.pcadapt(
  here("ustacks_populations_output", "EJA_whitelist_outliers", "populations.snps.vcf"),
  type = "vcf"
)
pcadapt_whitelist <- pcadapt(input = EJA_whitelist, K = 20)

plot(pcadapt_whitelist, option = "screeplot")
plot(pcadapt_whitelist, option = "scores", pop = poplist.names, i = 1, j = 2)
```

```{r pcadapt without Catalina}
EJA_all_loci_catalina <- read.pcadapt(here("ustacks_populations_output", "EJA_all_loci-catalian", "populations.snps.vcf"), type = "vcf") #Read vcf file for every loci

pcadapt_all_loci_catalina_raw <- pcadapt(input = EJA_all_loci_catalina, K = 20) #pcadapt analysis with high value of K
plot(pcadapt_all_loci_catalina_raw, option = "screeplot") #screeplot to visually chose pca component to keep

##K = 2 seems to be the most optimal for this data set

pcadapt_all_loci_catalina <- pcadapt(input = EJA_all_loci_catalina, K = 2)

poplist.names <- c(
  rep("carmel", 6),
  rep("monterey", 18),
  rep("palos_verdes", 9),
  rep("laguna", 9),
  rep("san_diego", 22),
  rep("punta_banda", 20),
  rep("cardon", 3)
) #map individuals to populations

plot(pcadapt_all_loci_catalina, option = "scores", pop = poplist.names, i = 1, j = 2)
100*pcadapt_all_loci_catalina $singular.values**2
plot(pcadapt_all_loci_catalina, option = "scores", pop = poplist.names, i = 2, j = 3)
plot(pcadapt_all_loci_catalina, option = "scores", pop = poplist.names, i = 1, j = 3)
```

```{r Outliers without Catalina}
#Getting catalog rank to vcf correspondence
EJA_all_loci_catalina_vcf <- read.vcfR(
  here("ustacks_populations_output", "EJA_all_loci-catalian", "populations.snps.vcf")
)
vcf_to_catalog_ranks_catalina.raw <- getID(EJA_all_loci_catalina_vcf) #get vcf catalog ranks
vcf_to_catalog_ranks_catalina.raw <- strsplit(x = vcf_to_catalog_ranks_catalina.raw, split = ':') #extract catalog ranks from other info

vcf_to_catalog_ranks_catalina <- rep(NA, length(vcf_to_catalog_ranks_catalina.raw))
for (i in 1:length(vcf_to_catalog_ranks_catalina)) {
  vcf_to_catalog_ranks_catalina[i] <- vcf_to_catalog_ranks_catalina.raw[[i]][1]
}
vcf_to_catalog_ranks_catalina <- as.numeric(vcf_to_catalog_ranks_catalina)


#Outliers selection by pcadapt pvalues
EJA_all_loci_catalina_padj <- p.adjust(pcadapt_all_loci_catalina$pvalues, method = "bonferroni")
alpha <- 0.1 #alpha chosen following https://bcm-uga.github.io/pcadapt/articles/pcadapt.html recommendations
EJA_catalina_outliers_vcf_ranks <- which(EJA_all_loci_catalina_padj < alpha)

#Getting phi_st from population analysis
phi_st_catalina <- read.table(file = here('ustacks_populations_output', 'EJA_all_loci-catalian', 'populations.phistats.tsv'), header = FALSE, sep = '\t', skip = 9, stringsAsFactors = FALSE)[,c(1,4)]
colnames(phi_st_catalina) <- c('catalog_rank', 'phi_st')

#Getting outliers catalog ranks
EJA_catalina_outliers_catalog_ranks <- vcf_to_catalog_ranks_catalina[EJA_catalina_outliers_vcf_ranks]

#Separating balancing selection from local adaptation (sorting outliers by phi_st)
EJA_catalina_outliers_phi_st <- phi_st_catalina[phi_st_catalina$catalog_rank %in% EJA_catalina_outliers_catalog_ranks, ]
dim(EJA_catalina_outliers_phi_st)[1]
EJA_catalina_outliers_phi_st_filtered <- EJA_catalina_outliers_phi_st[EJA_catalina_outliers_phi_st$phi_st > .1, ]
dim(EJA_catalina_outliers_phi_st_filtered)[1]


#Saving outliers list as txt file for stacks2 whitelist analysis
sink(here('output', 'EJA_catalina_outliers_catalog_ranks.txt'))
for (outlier in EJA_catalina_outliers_phi_st_filtered$catalog_rank){
  cat(paste0(outlier, '\n'), append = TRUE)
}
sink()
```

# Getting protein sequences for outliers

```{r Read the catalog and keep outliers sequences as fasta file}
#Read and prepare catalog fasta file
catalog_fa <- read.csv(file = here('data', 'catalog.fa'), stringsAsFactors = FALSE)
colnames(catalog_fa) <- c('Sequences')
catalog_sequences <- subset(catalog_fa, rownames(catalog.fa) %in% seq(1, dim(catalog_fa)[1], 2))
rownames(catalog_sequences) <- c(1:dim(catalog_sequences)[1])

length(EJA_outliers_catalog_ranks)#Plot number of outliers

outliers_sequences <- subset(catalog_sequences, rownames(catalog_sequences) %in% EJA_outliers_catalog_ranks) #Extracting sequences of outliers
head(outliers_sequences)
# write.csv(outliers_sequences, file = here('output', 'outliers_sequences.csv')) #If you want to save the outliers sequences as csv

#Writing results as .fa file for BLAST
sink(here('output', 'outliers_sequences.fa'))
for (i in 1:dim(outliers_sequences)[1]){
  cat(paste0('>', rownames(outliers_sequences)[i], '\n'), append = TRUE)
  cat(paste0(outliers_sequences$Sequences[i], '\n'), append = TRUE)
}
sink()
```

Please use the generated fasta file for NCBI BLAST analysis and download the result as Single-file XML2

```{r Read and extract relevant info from the BLAST results}
#Read BLAST results from xml file
hit.query <- read_xml(here('data', '8R1WY2TK01N-Alignment.xml')) #Replace second line of XML file by "<BlastXML2>" <=> Remove metadata from first xml tag.
end <- length(xml_find_all(hit.query, ".//BlastOutput2"))
counter <- 0
query_df <- data.frame(
  'query_number'     = NA,
  'hit_number'       = NA,
  'accession_number' = NA,
  'title'            = NA,
  'organism'         = NA,
  'bit_score'        = NA,
  'eval_number'      = NA
) #Data frame Initialisation
for (query in xml_find_all(hit.query, ".//BlastOutput2")){ #Reads XML BLAST results as data frame
  counter <- counter + 1
  query_id <- xml_text(xml_find_all(query, ".//query-title")) #Catalog rank
  print(paste0(counter, '/', end))
  for (Hit in xml_find_all(query, ".//Hit")){ #For each locus gets all hits
    num       <- xml_text(xml_find_all(Hit, './/num'))[1] #Rank of hit
    accession <- xml_text(xml_find_all(Hit, './/accession'))[1] #Accession number of the hit
    title     <- xml_text(xml_find_all(Hit, './/title'))[1] #Name of the hit
    organism  <- xml_text(xml_find_all(Hit, './/sciname'))[1] #Organism of the hit
    
    match_1   <- xml_find_all(Hit, './/Hsp')[1] #Sometime multiple match for one hit, keeps the best
    
    bit_score <- as.numeric(xml_text(xml_find_all(match_1, './/bit-score'))) #Get bit score
    evalue    <- as.numeric(xml_text(xml_find_all(match_1, './/evalue'))) #Get e-value
    
    query_df  <- rbind(query_df, c(query_id, num, accession, title, organism, bit_score, evalue))
  }
}
counter <- 0
query_df <- query_df[-1,] #Remove initialisation line
query_df$bit_score <- as.numeric(query_df$bit_score)
# saveRDS(query_df, file = here('output', 'Blast_results_as_data_frame.RDS'))
# query_df <- readRDS(here('output', 'Blast_results_as_data_frame.RDS')) #If you don't want to rerun the search...
```

```{r Filter the BLAST results}
query_df_80 <- query_df[query_df$bit_score >= 80, ] #Keep only significant matches
dim(query_df_80)[1]
query_df_80_PREDICTED <- query_df_80[grepl("PREDICTED", query_df_80$title), ] #keep only PREDICTED proteins. Removes non coding sequences and whole chromosome matches (causes problem when searching GeneBank)
dim(query_df_80_PREDICTED)[1]
query_df_80_PREDICTED_unique <- query_df_80_PREDICTED[!duplicated(query_df_80_PREDICTED$query_number), ] #Keep the first match for each locus
dim(query_df_80_PREDICTED_unique)[1]
query_df_80_PREDICTED_unique_unique <- query_df_80_PREDICTED_unique[!duplicated(query_df_80_PREDICTED_unique$accession_number), ] #Removes duplicated accession numbers
dim(query_df_80_PREDICTED_unique_unique)[1]

# write.csv(query_df_80_PREDICTED_unique_unique, file = here('output', 'query_df_80_PREDICTED_unique_unique.csv'))
```

```{r Get protein sequences from GenBank}
prot_sequences = data.frame(
  'protein_sequence' = rep(NA, length(query_df_80_PREDICTED_unique_unique$accession_number)),
  row.names          = query_df_80_PREDICTED_unique_unique$accession_number
) #Initialisation

pb = txtProgressBar(min = 0, max = dim(prot_sequences)[1], initial = 0) #Setup progress bar
for (i in 1:dim(prot_sequences)[1]){
  sequence_info <- entrez_fetch("nuccore", rownames(prot_sequences)[i], rettype ="fasta_cds_aa", retmode = "text") #Fetch protein info from GenBank from accession number
  prot_sequences[rownames(prot_sequences)[i], ] <- gsub(
    pattern     = '\n',
    replacement = '',
    x           = tail(unlist(strsplit(sequence_info, ']')),1)
  ) #Extract protein sequence from information
  setTxtProgressBar(pb, i) #Update progress bar
}
close(pb)

# prot_sequences = subset(x = prot_sequences, !(subset = prot_sequences$protein_sequence == ""))

#Write protein sequences as fasta file with GenBank access number as sequence names
sink(here('output', 'outliers_protein_sequences.fa'))
for (i in 1:dim(prot_sequences)[1]){
  cat(paste0('>', rownames(prot_sequences)[i], '\n'), append = TRUE)
  cat(paste0(prot_sequences$protein_sequence[i], '\n'), append = TRUE)
}
sink()
```

```{r Outliers protein match table}
Table_2 <- query_df_80_PREDICTED_unique_unique[, c('query_number', 'accession_number', 'title')]
colnames(Table_2)[1] <- 'Locus_ID'
write.csv(x = Table_2, file = here('output', 'Table_2.csv'), row.names = FALSE)
```

# Figures plotting

```{r Manhattan plot homemade}
#Getting chromosome map for manhatan plot
chromosome.names.raw <- getFIX(EJA_all_loci_vcf)[,"CHROM"] #Get chromosome for each vcf locus
chromosome.names     <- as.integer(lapply(X = chromosome.names.raw, FUN = gsub, pattern = 'SCAF_', replacement = '')) #Extract chromosome number from vcf info

#Getting x axis coordinates for chromosome id positions
mid_pos <- rep(NA, 24) #Initialise midrank vector
count_pos = as.data.frame(table(chromosome.names)) #Number of locus per chromosome

for (i in 1:24){
  mid_pos[i] <- Position(function(x) x == i, chromosome.names) + count_pos$Freq[i]/2
}

notNA.idx <- !is.na(pcadapt_all_loci$pvalues) 
chr.int <- chromosome.names%%2 #Colour mapping for chromosomes. Grey or black depending on parity

manhattan_df <- data.frame(
  x   = which(notNA.idx), #Catalog rank of locus with non-NA pcadapt p-value 
  y   = -as.numeric(
    pchisq(
      pcadapt_all_loci$chi2.stat[notNA.idx],
      df         = attr(pcadapt_all_loci, "K"),
      lower.tail = FALSE,
      log.p      = TRUE
    )/log(10)
  ), #Gets non adjusted -log10(pvalues) without infinite values
  chr = chr.int[notNA.idx] #Chromosome colour mapping
) 

manhattan_plot <- ggplot(manhattan_df, aes(x = x, y = y)) +
  geom_point(aes(colour = factor(manhattan_df$chr))) +
  guides(colour = FALSE) +
  scale_color_manual(values = c("black", "grey")) +
  scale_x_continuous(breaks = mid_pos, labels = 1:24) +
  labs(x = 'chromosome', y = '-log10(p-value)') +
  theme_bw() +
  theme(panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank())

ggsave(
  here('output', 'graphs', 'manhattan_plot.pdf'),
  plot = manhattan_plot,
  width = 16,
  height = 7
); system2('open', args = c('-a Preview.app', here('output', 'graphs', 'manhattan_plot.pdf')), wait = FALSE)

EJA_outliers_pvalues_rank <- data.frame(
  'vcf_rank'                 = EJA_outliers_vcf_ranks,
  'catalog_rank'             = as.character(EJA_outliers_catalog_ranks),
  'outliers_-log10(pvalues)' = -log10(pcadapt_all_loci$pvalues[EJA_outliers_vcf_ranks]),
  'outliers_pvalue_plot'     = manhattan_df$y[manhattan_df$x %in% EJA_outliers_vcf_ranks]
)

manhattan_plot_labels <- manhattan_plot +
  geom_label_repel(
    data    = EJA_outliers_pvalues_rank,
    mapping = aes(x = vcf_rank, y = outliers_pvalue_plot, label = catalog_rank)
  ) #Manhattan plot with outliers catalog ranks

ggsave(
  here('output', 'graphs', 'manhattan_plot_labels.pdf'),
  plot = manhattan_plot_labels,
  width = 16,
  height = 7
); system2('open', args = c('-a Preview.app', here('output', 'graphs', 'manhattan_plot_labels.pdf')), wait = FALSE)
```

```{r Figure PCA}
PCA_df <- data.frame('x' = -pcadapt_all_loci$scores[,1], 'y' = -pcadapt_all_loci$scores[,2], 'pop' = poplist.names)

PCA_percentages <- pcadapt_all_loci$singular.values^2
PCA_average_scores <- data.frame(
  "pop" = poplist.names,
  "1" = -pcadapt_all_loci$scores[, 1],
  "2" = -pcadapt_all_loci$scores[, 2]
)
PCA_average <- data.frame("pop" = levels(as.factor(poplist.names)))
for (i in 1:length(PCA_average$pop)){
  PCA_average$average_1[i] <- mean(PCA_average_scores[PCA_average_scores[, 1] == PCA_average$pop[i], 'X1'])
}
for (i in 1:length(PCA_average$pop)){
  PCA_average$average_2[i] <- mean(PCA_average_scores[PCA_average_scores[, 1] == PCA_average$pop[i], 'X2'])
}
PCA_average$pop <- c("Cardon", "Carmel", "Catalina", "Laguna Beach", "Monterey Bay", "Palos Verdes", "Punta Banda", "San Diego")
PCA_average$average_1 <- PCA_average$average_1 + c(0, -.02, .015, 0, 0, 0, 0, 0)
PCA_average$average_2 <- PCA_average$average_2 - c(.04, .01, -.01, .06, .06, .06, .055, .045)

PCA <- ggplot(PCA_df) +
  geom_point(mapping = aes(x = y, y = x, fill = pop), size = 3, shape = 21) +
  geom_text(data = PCA_average, mapping = aes(x = average_2, y = average_1, label = pop)) +
  labs(
    x = paste0('-PC2: ', as.character(round(PCA_percentages[2]*100, 2)), '%'),
    y = paste0('-PC1: ', as.character(round(PCA_percentages[1]*100, 2)), '%')
  ) +
  coord_fixed(ratio = 1.2) +
  theme(
    legend.position  = "none",
    panel.background = element_rect(fill = 'white', color = 'black'),
    panel.grid.major = element_line(color = 'black', linetype = 'dashed', linewidth = .12)
  ); PCA_map <- ggarrange(PCA, map, widths = c(1, 1.3))
```

```{r Figure PCA + Map}
#Extracting ocean surface temperature and current from BRAN2020 (https://dapds00.nci.org.au/thredds/catalog/gb6/BRAN/BRAN2020/month/catalog.html). See https://towardsdatascience.com/how-to-crack-open-netcdf-files-in-r-and-extract-data-as-time-series-24107b70dcd for tutorial on NetCDF4 files
nc_temp <- nc_open(here('data','ocean_temp_ann_2022.nc')) #Longitude's 0 is at Greenwich and -180 become 3600. Latitudes start at -75 and end at +75. To convert add +360 to lon and +75 to lat
nc_u   <- nc_open(here('data', 'ocean_u_ann_2022.nc'))
nc_v   <- nc_open(here('data', 'ocean_v_ann_2022.nc'))

#Converting arrays to ata frame
lon <- ncvar_get(nc_temp, "xt_ocean")
lon[1801:3600] <- seq(-179.95,-0.05,.1)#Converting longitude format to sample and lanmass coordiantes
lat <- ncvar_get(nc_temp, "yt_ocean") 
lonlat <- as.matrix(expand.grid(lon,lat))

temp_array  <- ncvar_get(nc_temp, "temp")
temp_matrix <- as.matrix(temp_array[,,1])
temp_vector <- as.vector(temp_matrix)

u_array  <- ncvar_get(nc_u, "u")
u_matrix <- as.matrix(u_array[,,1])
u_vector <- as.vector(u_matrix)

v_array  <- ncvar_get(nc_v, "v")
v_matrix <- as.matrix(v_array[,,1])
v_vector <- as.vector(v_matrix)

#Final data frame for whold world
world_map_data_df <- data.frame(cbind(lonlat, temp_vector, u_vector, v_vector))
colnames(world_map_data_df) <- c('lon', 'lat', 'temp', 'u', 'v')

#Crop to smaller size for diminished render time
sub_lonlat <- as.matrix(expand.grid(lon[(3600-1300):(3600-1100)], lat[(200+750):(400+750)]))
sub_temp   <- as.vector(temp_matrix[(3600-1300):(3600-1100),(200+750):(400+750)])
sub_u      <- as.vector(u_matrix[(3600-1300):(3600-1100),(200+750):(400+750)])
sub_v      <- as.vector(v_matrix[(3600-1300):(3600-1100),(200+750):(400+750)])

#Reduce current speed vecotr density
sub_u_lowres <- replace(sub_u, 2*c(1:(length(sub_u)/2)), NA)
sub_v_lowres <- replace(sub_v, 2*c(1:(length(sub_u)/2)), NA)

map_data_lowres_df <- data.frame(cbind(sub_lonlat, sub_temp, sub_u_lowres, sub_v_lowres))
colnames(map_data_lowres_df) <- c('lon', 'lat', 'temp', 'u', 'v')

current_uv_scalar <- 2 #Set scalar for better vector vivisbility, don't forget when setting scale in final graph

#Setting up landmass map from naturalearth
world  <- ne_countries(scale = "large", country = c('united states of america', 'mexico'), returnclass = "sf")
states <- ne_states(country = c('united states of america', 'mexico'), returnclass = 'sf')

#Sampling site coordinates and misc info on map
sample_coords <- read.csv(here('data','EJA_RAD_Samples.csv'), stringsAsFactors = FALSE)
sample_sites  <- sample_coords[!duplicated(sample_coords$Sampling.location), ]
sample_sites$lab_lon <- c(-121.9170, -121.9348, -118.4126, -117.7821, -117.2420, -116.7030, -114.4550, -118.4163) +
  .15 +
  c(0, 0, 0, .1, 0, 0, 0, -.1 -2)
sample_sites$lab_lat <- c(36.63045, 36.53834, 33.74077, 33.53716, 32.76270, 31.71139, 28.90742, 33.38790) +
  .25 +
  c(0, -.5, 0, -.2, 0, 0, 0, -.1 -1)
states_labels <- data.frame(
  'long' = c(-117.5, -115.5, -113.7, -115.2),
  'lat'  = c(35.5, 36.6, 35, 30.25),
  'lab'  = c('CA', 'NV', 'AZ', 'BC')
)
city_labels <- data.frame(
  'lat'      = c(37.773972, 34.448113),
  'long'     = c(-122.431297, -120.471439),
  'lat_lab'  = c(37.9, 34.8),
  'long_lab' = c(-122.28, -120.535),
  'lab'      = c('San Francisco', 'Point\nConception')
)
ocean_labels <- data.frame(
  'long' = c(-121,-113.85),
  'lat'  = c(31,30.7),
  'lab'  = c('Pacific Ocean','Gulf\nof\nCalifornia')
)


map <- ggplot() +
  #Temperature map (https://theoceancode.netlify.app/post/mapping_with_ggplot2/)
  geom_tile(data = map_data_lowres_df, aes(x = lon, y = lat, fill = temp)) +
  scale_fill_viridis(name = 'Temp.\n(¬∞C)', option = 'turbo') +
  #Current vectors (https://theoceancode.netlify.app/post/mapping_with_ggplot2/)
  geom_segment(
    data    = map_data_lowres_df,
    mapping = aes(x = lon, y = lat, xend = lon + u * current_uv_scalar, yend = lat + v * current_uv_scalar),
    arrow   = arrow(angle = 15, length = unit(0.04, "inches"), type = "closed"),
    na.rm   = TRUE,
    alpha   = 0.3
  ) +
  #Landmass
  geom_sf(data = states) +
  #Sampling sites points and labels
  new_scale("fill") +
  geom_path(
    data    = data.frame('x' = c(-118.4163, -118.4163 -.85), 'y' = c(33.38790, 33.38790 -.73)),
    mapping = aes(x, y)
  ) +
  geom_point(
    sample_sites,
    mapping     = aes(x = Longitude, y = Latitude, fill = Sampling.location, size = 1.3),
    shape       = 21,
    show.legend = FALSE
  ) +
  geom_label(sample_sites, mapping = aes(x = lab_lon, y = lab_lat, label = Sampling.location), hjust = 0) +
  #Extra cities points and labels
  geom_point(city_labels, mapping = aes(x = long, y = lat), size = 4, shape = 20, show.legend = FALSE) +
  geom_text(city_labels, mapping = aes(x = long_lab, y = lat_lab, label = lab), lineheight = .7, hjust = 0) +
  #Ocean and state names
  geom_text(states_labels, mapping = aes(x = long, y = lat, label = lab), size = 5.5) +
  geom_text(ocean_labels[2,], mapping = aes(x = long, y = lat, label = lab), colour = 'ivory', size = 5) +
  geom_label(
    ocean_labels[1,],
    mapping       = aes(x = long, y = lat, label = lab),
    colour        = 'black',
    size          = 5,
    label.padding = unit(0.5, "lines")
  ) +
  #Crop map 
  coord_sf(xlim = c(-124, -113), ylim = c(28.5, 38.5), expand = FALSE) +
  #Vector, temperature and distance scale + orientation
  geom_label(aes(x = -122, y = 29.5, label = "1.0 m/s\n"), size = 4.5, label.padding = unit(0.5, "lines")) +
  geom_segment(
    aes(x = -122.5, y = 29.25, xend = -121.5, yend = 29.25),
    arrow = arrow(angle = 15, length = unit(0.04, "inches"), type = "closed")
  ) +
  geom_label(aes(x = -119.1, y = 28.71, label = '        ')) +
  annotation_scale(location = "bl", width_hint = 0.4) + #Scale bar is not perfectly accurate because of surface covered
  annotation_north_arrow(location = "tr", style = north_arrow_fancy_orienteering()) +
  #Set text around map and coordinates display
  labs(x = 'Longitude', y = 'Latitude') +
  scale_x_continuous(breaks = c(-120, -115), labels = c('120¬∞W','115¬∞W')) +
  scale_y_continuous(breaks = c(30, 35), labels = unit_format(suffix = "¬∞N", sep = ""))

ggsave(
  here('output', 'graphs', 'temp_current_map.pdf'),
  plot   = map,
  width  = 9,
  height = 9
); system2('open', args = c('-a Preview.app', here('output', 'graphs', 'temp_current_map.pdf')), wait = FALSE)

PCA_map <- ggarrange(PCA, map, widths = c(1, 1.3))

ggsave(
  here('output', 'graphs', 'pca_map.pdf'),
  plot   = PCA_map,
  width  = 14,
  height = 9
); system2('open', args = c('-a Preview.app', here('output', 'graphs', 'pca_map.pdf')), wait = FALSE)
```

```{r Generating bathymetry map}
catalina <- getNOAA.bathy(lon1 = -121, lon2 = -116, lat1 = 32, lat2 = 35, resolution = .5) #Get bathymetry data
catalina_df <- fortify.bathy(catalina)
catalina_df$z[catalina_df$z>0] <- NA #Remove land data

map_data_df <- data.frame(cbind(sub_lonlat, sub_u, sub_v)) #Get surface current vectors
colnames(map_data_df) <- c('lon', 'lat', 'u', 'v')

bathy <- ggplot() +
  #Plot bathymetry
  geom_raster(data = catalina_df, mapping = aes(x = x, y = y, fill = z)) +
  #Plot landmass
  geom_sf(data = states) +
  #Plot surface current speed
  geom_segment(
    data    = map_data_df,
    mapping = aes(x = lon, y = lat, xend = lon + u * 1, yend = lat + v * 1),
    arrow   = arrow(angle = 15, length = unit(0.04, "inches"), type = "closed"),
    na.rm   = TRUE,
    alpha   = 0.3
  ) +
  #Plot sampling sites
  geom_point(
    sample_sites[c(3,4,5,8),],
    mapping     = aes(x = Longitude, y = Latitude, size = 1.3),
    show.legend = FALSE
  ) +
  geom_label(
    sample_sites[c(3,4,5,8),],
    mapping = aes(x = Longitude, y = Latitude, label = Sampling.location),
    hjust   = 0,
    nudge_x = .05,
    nudge_y = .1
  ) +
  #Plot extra landmark
  geom_point(city_labels[2, ], mapping = aes(x = long, y = lat), size = 4, shape = 20, show.legend = FALSE) +
  geom_text(
    city_labels[2, ],
    mapping = aes(x = long, y = lat, label = lab),
    lineheight = .7,
    hjust = 0,
    nudge_x = .05,
    nudge_y = .1
  ) +
  #Plot scales
  scale_fill_gradient(name = 'Depth (m)', low = "dodgerblue4",high="gainsboro") +
  geom_label(aes(x = -120.4, y = 32.3, label = ".25 m/s\n"), size = 4.5, label.padding = unit(0.5, "lines")) +
  geom_segment(
    aes(x = -120.52, y = 32.25, xend = -120.27, yend = 32.25),
    arrow = arrow(angle = 15, length = unit(0.04, "inches"), type = "closed")
  ) +
  geom_label(aes(x = -119.705, y = 32.08, label = '        ')) +
  annotation_scale(location = "bl", width_hint = 0.4) +
  annotation_north_arrow(location = "tr", style = north_arrow_fancy_orienteering()) +
  #Crop map
  coord_sf(xlim = c(-121, -116), ylim = c(32, 35), expand = FALSE) +
  #Axis labels and theme
  labs(x = 'Longitude', y = 'Latitude') +
  scale_y_continuous(breaks = seq(32, 35, 1), labels = unit_format(suffix = "¬∞N", sep = "")) +
  theme(legend.position = c(.93, .73), plot.margin = margin(t = 0, r = 1, b = 0, l = .5, unit = 'cm'))
  
ggsave(
  here('output', 'graphs', 'bathy.pdf'),
  plot = bathy,
  width = 9,
  height = 7
); system2('open', args = c('-a Preview.app', here('output', 'graphs', 'bathy.pdf')), wait = FALSE)
```

```{r Generating map without specific background}
map_blank <- ggplot() +
  #Landmass
  geom_sf(data = states) +
  #Sampling sites points and labels
  new_scale("fill") +
  geom_path(
    data = data.frame('x' = c(-118.4163, -118.4163 -.85), 'y' = c(33.38790, 33.38790 -.73)),
    mapping = aes(x, y)
  ) +
  geom_point(
    sample_sites,
    mapping = aes(x = Longitude, y = Latitude, fill = Sampling.location, size = 1.3),
    shape = 21,
    show.legend = FALSE
  ) +
  geom_label(sample_sites, mapping = aes(x = lab_lon, y = lab_lat, label = Sampling.location), hjust = 0) +
  geom_point(city_labels, mapping = aes(x = long, y = lat), size = 4, shape = 20, show.legend = FALSE) +
  geom_text(city_labels, mapping = aes(x = long_lab, y = lat_lab, label = lab), lineheight = .7, hjust = 0) +
  #Ocean and state names
  geom_text(states_labels, mapping = aes(x = long, y = lat, label = lab), size = 5.5) +
  geom_text(ocean_labels[2,], mapping = aes(x = long, y = lat, label = lab), colour = 'ivory', size = 5) +
  geom_label(
    ocean_labels[1,],
    mapping = aes(x = long, y = lat, label = lab),
    colour = 'black',
    size = 5,
    label.padding = unit(0.5, "lines")
  ) +
  #Crop map 
  coord_sf(xlim = c(-124, -113), ylim = c(28.5, 38.5), expand = FALSE) +
  #Vector, temperature and distance scale + orientation
  geom_label(aes(x = -119.1, y = 28.71, label = '        ')) +
  annotation_scale(location = "bl", width_hint = 0.4) + #Scale bar is not perfectly accurate because of surface covered
  annotation_north_arrow(location = "tr", style = north_arrow_fancy_orienteering()) +
  #Set text around map and coordinates display
  labs(x = 'Longitude', y = 'Latitude') +
  scale_x_continuous(breaks = c(-120, -115), labels = c('120¬∞W','115¬∞W')) +
  scale_y_continuous(breaks = c(30, 35), labels = unit_format(suffix = "¬∞N", sep = "")) +
  theme(panel.background = element_rect('lightblue'))
ggsave(
  here('output', 'graphs', 'map_blank.pdf'),
  plot = map_blank,
  width = 7.5,
  height = 7.5
); system2('open', args = c('-a Preview.app', here('output', 'graphs', 'map_blank.pdf')), wait = FALSE)
```

```{r Convert blast KOALA pie piechart in barplot}
koala_results <- read.csv(here('external_results', 'Blast_koala_results.csv'), stringsAsFactors = FALSE)
koala_results$Functional.category <- factor(koala_results$Functional.category, koala_results$Functional.category)
koala_ggplot <- ggplot(
  data = koala_results,
  mapping = aes(x = Functional.category, y = Counts, fill = Functional.category)
  ) +
  geom_col() +
  # scale_fill_discrete(name = 'Functional category') +
  scale_fill_manual(
    name = 'Functional category',
    labels = koala_results$Functional.category,
    breaks = koala_results$Functional.category,
    values = koala_results$colour
  ) +
  theme(
    panel.background = element_rect(fill = 'white', colour = 'black'),
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    panel.grid.major.y = element_line(colour = 'black', linetype = 'solid', linewidth = .12),
    panel.grid.minor.y = element_line(colour = 'black', linetype = 'dashed', linewidth = .08),
    axis.ticks.x = element_blank(),
    axis.text.x = element_blank(),
    axis.title.x = element_blank()
  )
ggsave(
  filename = here('external_results', 'blast_koala_bar_plot.pdf'),
  plot = koala_ggplot,
  width = 21,
  height = 10,
  units = 'cm'
)
```

# FST tree stuff

```{r Functions}
read.tsv.genes <- function(path, names) {#Allows to read tsv files from structures and add populations names
  tsv              <- read.csv(path, sep = '\t')
  tsv_t            <- t(rbind(tsv[,-1], rep(NA, 8)))
  row.names(tsv_t) <- names
  colnames(tsv_t)  <- names
  return(tsv_t)
}

fix_negative_edge_length <- function(nj.tree) {
  edge_infos           <- cbind(nj.tree$edge, nj.tree$edge.length) %>% as.data.table
  colnames(edge_infos) <- c('from', 'to', 'length')
  nega_froms           <- edge_infos[length < 0, sort(unique(from))]
  nega_froms
  for (nega_from in nega_froms) {
    minus_length <- edge_infos[from == nega_from, ][order(length)][1, length]
    edge_infos[from == nega_from, length := length - minus_length]
    edge_infos[to == nega_from, length := length + minus_length]
  }
  nj.tree$edge.length <- edge_infos$length
  nj.tree
}

# fix_negative_edge_length <- function(nj.tree) {#Fix negative length for NJ tree branches length
#   edge_infos <- data.frame(cbind(nj.tree$edge, nj.tree$edge.length))
#   # %>% as.data.table
#   colnames(edge_infos) <- c('from', 'to', 'length')
#   nega_froms <- sort(unique(edge_infos[edge_infos$length < 0,]$from))
#   nega_froms
#   for (nega_from in nega_froms) {
#     minus_length <- edge_infos[from == nega_from, ][order(length)][1, length]
#     edge_infos[from == nega_from, length := length - minus_length]
#     edge_infos[to == nega_from, length := length + minus_length]
#   }
#   nj.tree$edge.length <- edge_infos$length
#   nj.tree
# }

```

```{r FST tree projection on map}
FST_df <- read.csv(here('data', 'FST_blacklist.csv'))
FST_matrix <- as.matrix(read.csv(here('data', 'FST_blacklist.csv'), row.names = 'Name'))
FST_melted <- melt(FST_matrix)

ggplot(FST_melted, aes(x = Var1, y = Var2, fill = value )) + geom_tile()

FST_matrix_tree <- nj(FST_matrix)

FST_matrix_lt <- as.matrix(read.csv(here('data', 'FST_blacklist_lt.csv'), row.names = 'Name'))
FST_matrix_lt_tree <- nj(as.dist(FST_matrix_lt))

FST_matrix_small <- as.matrix(read.csv(here('data', 'FST_blacklist_small.csv'), row.names = 'Name'))
FST_matrix_small_tree <- nj(as.dist(FST_matrix_small))

names = c("Carmel", "Monterey", "Palos_Verdes", "Laguna_beach", "Catalina", "San_Diego", "Punta_banda", "Cardon")

POP_FST_tree <- nj(as.dist(read.tsv.genes(here('all_loci','EJA_blacklist','populations.fst_summary.tsv'), names)))

POP_Phi_tree <- nj(as.dist(read.tsv.genes(here('data', 'Phi_st_Means.tsv'), names)))

POP_FST_prime <- nj(as.dist(read.tsv.genes(here('data', 'Fst_prime_Means.tsv'), names)))

## plot(FST_matrix_tree)
plot(FST_matrix_lt_tree)
plot(FST_matrix_small_tree)
plot(POP_FST_tree)
plot(POP_Phi_tree)
plot(POP_FST_prime)

# plot(FST_matrix_lt_tree)
plot(fix_negative_edge_length(FST_matrix_lt_tree))
# plot(FST_matrix_small_tree)
plot(fix_negative_edge_length(POP_FST_tree))
plot(fix_negative_edge_length(POP_Phi_tree))
plot(fix_negative_edge_length(POP_FST_prime))
```
